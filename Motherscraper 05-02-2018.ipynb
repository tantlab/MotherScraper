{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error sending mail\n",
      "error sending mail\n",
      "Scraping 85891659096 page 1 of 3\n",
      "Fetching data\n",
      "PostID: 85891659096_10152061499867225 from 2013-12-26T19:52:47+0000\n",
      "Posts scraped 1\n",
      "PostID: 85891659096_10152506479204097 from 2013-12-26T08:00:01+0000\n",
      "Posts scraped 2\n",
      "PostID: 85891659096_10152505333799097 from 2013-12-25T22:12:45+0000\n",
      "Posts scraped 3\n",
      "PostID: 85891659096_10152491231359097 from 2013-12-20T08:51:01+0000\n",
      "Posts scraped 4\n",
      "PostID: 85891659096_10152487119629097 from 2013-12-18T14:00:02+0000\n",
      "Posts scraped 5\n",
      "PostID: 85891659096_10152486722009097 from 2013-12-18T08:00:01+0000\n",
      "Posts scraped 6\n",
      "PostID: 85891659096_10152484827859097 from 2013-12-17T14:00:01+0000\n",
      "Posts scraped 7\n",
      "PostID: 85891659096_10152484379994097 from 2013-12-17T08:00:01+0000\n",
      "Posts scraped 8\n",
      "PostID: 85891659096_10152482402789097 from 2013-12-16T14:00:01+0000\n",
      "Posts scraped 9\n",
      "PostID: 85891659096_10152481941124097 from 2013-12-16T08:00:01+0000\n",
      "Posts scraped 10\n",
      "PostID: 85891659096_10152479764164097 from 2013-12-15T14:00:01+0000\n",
      "Posts scraped 11\n",
      "PostID: 85891659096_10152479499614097 from 2013-12-15T11:12:35+0000\n",
      "Posts scraped 12\n",
      "PostID: 85891659096_10152479199679097 from 2013-12-12T21:32:21+0000\n",
      "Posts scraped 13\n",
      "PostID: 85891659096_10202448639714439 from 2013-12-14T22:30:59+0000\n",
      "Posts scraped 14\n",
      "PostID: 85891659096_10152473792389097 from 2013-12-13T14:00:01+0000\n",
      "Posts scraped 15\n",
      "PostID: 85891659096_10152473707869097 from 2013-12-13T06:00:01+0000\n",
      "Posts scraped 16\n",
      "PostID: 85891659096_10152472836619097 from 2013-12-12T21:14:01+0000\n",
      "Posts scraped 17\n",
      "PostID: 85891659096_10152470132204097 from 2013-12-11T19:55:23+0000\n",
      "Posts scraped 18\n",
      "PostID: 85891659096_10152469186919097 from 2013-12-11T13:04:37+0000\n",
      "Posts scraped 19\n",
      "PostID: 85891659096_10152466887919097 from 2013-12-10T15:00:01+0000\n",
      "Posts scraped 20\n",
      "Finished first post page.\n",
      "Paging posts for 85891659096\n",
      "Fetching data\n",
      "PostID: 85891659096_10152463742889097 from 2013-12-09T15:00:01+0000\n",
      "Posts scraped 21\n",
      "PostID: 85891659096_10152461383299097 from 2013-12-08T19:10:58+0000\n",
      "Posts scraped 22\n",
      "PostID: 85891659096_10152460862279097 from 2013-12-08T15:00:01+0000\n",
      "Posts scraped 23\n",
      "PostID: 85891659096_10152458272279097 from 2013-12-07T15:00:01+0000\n",
      "Posts scraped 24\n",
      "PostID: 85891659096_10152455568029097 from 2013-12-06T15:00:01+0000\n",
      "Posts scraped 25\n",
      "PostID: 85891659096_10152453403639097 from 2013-12-05T20:42:33+0000\n",
      "Posts scraped 26\n",
      "PostID: 85891659096_10152452573879097 from 2013-12-05T14:55:18+0000\n",
      "Posts scraped 27\n",
      "PostID: 85891659096_10152444974034097 from 2013-12-02T20:51:44+0000\n",
      "Posts scraped 28\n",
      "PostID: 85891659096_10152441727204097 from 2013-12-01T18:33:15+0000\n",
      "Posts scraped 29\n",
      "PostID: 85891659096_10152435426274097 from 2013-11-29T09:04:35+0000\n",
      "Posts scraped 30\n",
      "PostID: 85891659096_10152435376039097 from 2013-11-29T08:28:08+0000\n",
      "Posts scraped 31\n",
      "PostID: 85891659096_10152433325759097 from 2013-11-28T15:00:01+0000\n",
      "Posts scraped 32\n",
      "PostID: 85891659096_10152432776974097 from 2013-11-28T09:00:01+0000\n",
      "Posts scraped 33\n",
      "PostID: 85891659096_10152430758139097 from 2013-11-27T15:00:01+0000\n",
      "Posts scraped 34\n",
      "PostID: 85891659096_10152430624174097 from 2013-11-27T13:49:32+0000\n",
      "Posts scraped 35\n",
      "PostID: 85891659096_10152430243484097 from 2013-11-27T10:03:41+0000\n",
      "Posts scraped 36\n",
      "PostID: 85891659096_10152428600674097 from 2013-11-27T06:00:00+0000\n",
      "Posts scraped 37\n",
      "PostID: 85891659096_10152430233899097 from 2013-11-26T20:36:39+0000\n",
      "Posts scraped 38\n",
      "PostID: 85891659096_10152428525544097 from 2013-11-26T19:44:40+0000\n",
      "Posts scraped 39\n",
      "PostID: 85891659096_10152427185629097 from 2013-11-26T08:19:38+0000\n",
      "Posts scraped 40\n",
      "Fetching data\n",
      "PostID: 85891659096_10152417872884097 from 2013-11-23T09:00:01+0000\n",
      "Posts scraped 41\n",
      "PostID: 85891659096_10152416062409097 from 2013-11-22T17:30:02+0000\n",
      "Posts scraped 42\n",
      "PostID: 85891659096_10152415602569097 from 2013-11-22T14:00:01+0000\n",
      "Posts scraped 43\n",
      "PostID: 85891659096_10152424011504097 from 2013-11-21T22:09:08+0000\n",
      "Posts scraped 44\n",
      "PostID: 85891659096_10152410948224097 from 2013-11-20T23:55:34+0000\n",
      "Posts scraped 45\n",
      "PostID: 85891659096_10152409743334097 from 2013-11-20T14:51:29+0000\n",
      "Posts scraped 46\n",
      "PostID: 85891659096_10152410167759097 from 2013-11-20T13:25:41+0000\n",
      "Posts scraped 47\n",
      "PostID: 85891659096_10152407082594097 from 2013-11-19T16:35:04+0000\n",
      "Posts scraped 48\n",
      "PostID: 85891659096_10152404755419097 from 2013-11-18T19:55:52+0000\n",
      "Posts scraped 49\n",
      "PostID: 85891659096_10152404032134097 from 2013-11-18T14:13:11+0000\n",
      "Posts scraped 50\n",
      "PostID: 85891659096_10152401192869097 from 2013-11-17T14:00:02+0000\n",
      "Posts scraped 51\n",
      "PostID: 85891659096_10152400734264097 from 2013-11-17T09:00:01+0000\n",
      "Posts scraped 52\n",
      "PostID: 85891659096_10152398388639097 from 2013-11-16T14:00:00+0000\n",
      "Posts scraped 53\n",
      "PostID: 85891659096_10152397944534097 from 2013-11-16T09:00:01+0000\n",
      "Posts scraped 54\n",
      "PostID: 85891659096_10152396763644097 from 2013-11-15T21:14:56+0000\n",
      "Posts scraped 55\n",
      "PostID: 85891659096_10152393681024097 from 2013-11-14T18:26:56+0000\n",
      "Posts scraped 56\n",
      "PostID: 85891659096_10152393313679097 from 2013-11-14T16:14:57+0000\n",
      "Posts scraped 57\n",
      "PostID: 85891659096_10152390417929097 from 2013-11-13T17:30:01+0000\n",
      "Posts scraped 58\n",
      "PostID: 85891659096_10152390144654097 from 2013-11-13T14:51:19+0000\n",
      "Posts scraped 59\n",
      "PostID: 85891659096_10152390115754097 from 2013-11-13T14:31:19+0000\n",
      "Posts scraped 60\n",
      "Fetching data\n",
      "PostID: 85891659096_10152387890834097 from 2013-11-12T16:32:56+0000\n",
      "Posts scraped 61\n",
      "PostID: 85891659096_10152387832134097 from 2013-11-12T15:57:38+0000\n",
      "Posts scraped 62\n",
      "PostID: 85891659096_10152387450504097 from 2013-11-12T12:17:57+0000\n",
      "Posts scraped 63\n",
      "PostID: 85891659096_10152380886279097 from 2013-11-09T22:53:25+0000\n",
      "Posts scraped 64\n",
      "PostID: 85891659096_10152380854049097 from 2013-11-09T22:37:25+0000\n",
      "Posts scraped 65\n",
      "PostID: 85891659096_10152375933679097 from 2013-11-07T19:10:55+0000\n",
      "Posts scraped 66\n",
      "PostID: 85891659096_10152368670984097 from 2013-11-06T23:05:22+0000\n",
      "Posts scraped 67\n",
      "PostID: 85891659096_10152363697074097 from 2013-11-05T17:30:01+0000\n",
      "Posts scraped 68\n",
      "PostID: 85891659096_10152363132764097 from 2013-11-05T13:27:54+0000\n",
      "Posts scraped 69\n",
      "PostID: 85891659096_10152357886299097 from 2013-11-03T18:23:48+0000\n",
      "Posts scraped 70\n",
      "PostID: 85891659096_10152348139039097 from 2013-10-31T21:33:35+0000\n",
      "Posts scraped 71\n",
      "PostID: 85891659096_10152346569204097 from 2013-10-31T09:44:29+0000\n",
      "Posts scraped 72\n",
      "PostID: 85891659096_10152344061714097 from 2013-10-30T15:26:20+0000\n",
      "Posts scraped 73\n",
      "PostID: 85891659096_10152344023819097 from 2013-10-30T15:08:11+0000\n",
      "Posts scraped 74\n",
      "PostID: 85891659096_10152340809214097 from 2013-10-29T14:09:26+0000\n",
      "Posts scraped 75\n",
      "PostID: 85891659096_10152340785739097 from 2013-10-29T13:57:23+0000\n",
      "Posts scraped 76\n",
      "PostID: 85891659096_10152340766029097 from 2013-10-29T13:46:43+0000\n",
      "Posts scraped 77\n",
      "PostID: 85891659096_10152337361569097 from 2013-10-28T14:16:12+0000\n",
      "Posts scraped 78\n",
      "PostID: 85891659096_10152337286179097 from 2013-10-28T13:38:12+0000\n",
      "Posts scraped 79\n",
      "PostID: 85891659096_10152337206989097 from 2013-10-28T13:02:35+0000\n",
      "Posts scraped 80\n",
      "Fetching data\n",
      "PostID: 85891659096_10152336897099097 from 2013-10-28T09:49:04+0000\n",
      "Posts scraped 81\n",
      "PostID: 85891659096_10152332575519097 from 2013-10-26T21:59:22+0000\n",
      "Posts scraped 82\n",
      "PostID: 85891659096_10152330963944097 from 2013-10-26T11:40:52+0000\n",
      "Posts scraped 83\n",
      "PostID: 85891659096_10152328026079097 from 2013-10-25T09:09:56+0000\n",
      "Posts scraped 84\n",
      "PostID: 85891659096_10152328016809097 from 2013-10-25T08:59:04+0000\n",
      "Posts scraped 85\n",
      "PostID: 85891659096_10152327959904097 from 2013-10-25T08:02:41+0000\n",
      "Posts scraped 86\n",
      "PostID: 85891659096_10201384663071643 from 2013-10-24T15:54:31+0000\n",
      "Posts scraped 87\n",
      "PostID: 85891659096_10152323446369097 from 2013-10-23T14:04:45+0000\n",
      "Posts scraped 88\n",
      "PostID: 85891659096_10152323020569097 from 2013-10-23T08:53:53+0000\n",
      "Posts scraped 89\n",
      "PostID: 85891659096_10152323019834097 from 2013-10-23T08:53:01+0000\n",
      "Posts scraped 90\n",
      "PostID: 85891659096_10152321194839097 from 2013-10-22T19:44:17+0000\n",
      "Posts scraped 91\n",
      "PostID: 85891659096_10152320087789097 from 2013-10-22T10:41:06+0000\n",
      "Posts scraped 92\n",
      "PostID: 85891659096_10152317912369097 from 2013-10-21T20:02:05+0000\n",
      "Posts scraped 93\n",
      "PostID: 85891659096_10151952736147165 from 2013-10-21T16:29:47+0000\n",
      "Posts scraped 94\n",
      "PostID: 85891659096_10152317235954097 from 2013-10-21T15:11:29+0000\n",
      "Posts scraped 95\n",
      "PostID: 85891659096_10152317217859097 from 2013-10-21T15:02:30+0000\n",
      "Posts scraped 96\n",
      "PostID: 85891659096_10152316876159097 from 2013-10-21T08:45:46+0000\n",
      "Posts scraped 97\n",
      "PostID: 85891659096_10152315013619097 from 2013-10-20T17:51:54+0000\n",
      "Posts scraped 98\n",
      "PostID: 85891659096_10152314871969097 from 2013-10-20T17:05:00+0000\n",
      "Posts scraped 99\n",
      "PostID: 85891659096_10152314868504097 from 2013-10-20T17:03:09+0000\n",
      "Posts scraped 100\n",
      "Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostID: 85891659096_10152314863964097 from 2013-10-20T17:00:33+0000\n",
      "Posts scraped 101\n",
      "PostID: 85891659096_10202435517755114 from 2013-10-20T07:17:16+0000\n",
      "Posts scraped 102\n",
      "PostID: 85891659096_10152310672944097 from 2013-10-19T07:08:45+0000\n",
      "Posts scraped 103\n",
      "PostID: 85891659096_10152306519929097 from 2013-10-17T16:52:12+0000\n",
      "Posts scraped 104\n",
      "PostID: 85891659096_10152306503679097 from 2013-10-17T16:42:14+0000\n",
      "Posts scraped 105\n",
      "PostID: 85891659096_10152306497369097 from 2013-10-17T16:38:26+0000\n",
      "Posts scraped 106\n",
      "PostID: 85891659096_10152303532804097 from 2013-10-16T08:11:03+0000\n",
      "Posts scraped 107\n",
      "PostID: 85891659096_10152303519844097 from 2013-10-16T07:58:05+0000\n",
      "Posts scraped 108\n",
      "PostID: 85891659096_10152303429099097 from 2013-10-16T06:27:01+0000\n",
      "Posts scraped 109\n",
      "PostID: 85891659096_10152303362184097 from 2013-10-16T05:32:27+0000\n",
      "Posts scraped 110\n",
      "PostID: 85891659096_10152301161974097 from 2013-10-15T10:36:41+0000\n",
      "Posts scraped 111\n",
      "PostID: 85891659096_10152301036729097 from 2013-10-15T09:48:04+0000\n",
      "Posts scraped 112\n",
      "PostID: 85891659096_10152301031254097 from 2013-10-15T09:40:29+0000\n",
      "Posts scraped 113\n",
      "PostID: 85891659096_10152290463884097 from 2013-10-11T12:58:22+0000\n",
      "Posts scraped 114\n",
      "PostID: 85891659096_10152290451669097 from 2013-10-11T12:49:04+0000\n",
      "Posts scraped 115\n",
      "PostID: 85891659096_10152288480579097 from 2013-10-10T14:16:20+0000\n",
      "Posts scraped 116\n",
      "PostID: 85891659096_10152285377464097 from 2013-10-09T09:42:45+0000\n",
      "Posts scraped 117\n",
      "PostID: 85891659096_10152283249154097 from 2013-10-08T14:38:41+0000\n",
      "Posts scraped 118\n",
      "PostID: 85891659096_10152280998784097 from 2013-10-07T18:00:04+0000\n",
      "Posts scraped 119\n",
      "PostID: 85891659096_10152278496339097 from 2013-10-06T17:02:21+0000\n",
      "Posts scraped 120\n",
      "Fetching data\n",
      "PostID: 85891659096_10152275576784097 from 2013-10-05T09:58:57+0000\n",
      "Posts scraped 121\n",
      "PostID: 85891659096_10152274039799097 from 2013-10-04T16:43:04+0000\n",
      "Posts scraped 122\n",
      "PostID: 85891659096_10152269124359097 from 2013-10-02T12:42:25+0000\n",
      "Posts scraped 123\n",
      "PostID: 85891659096_10152266952974097 from 2013-10-01T14:03:25+0000\n",
      "Posts scraped 124\n",
      "PostID: 85891659096_10152264161479097 from 2013-09-30T11:26:43+0000\n",
      "Posts scraped 125\n",
      "PostID: 85891659096_10152264527489097 from 2013-09-30T13:26:37+0000\n",
      "Posts scraped 126\n",
      "PostID: 85891659096_10152261324354097 from 2013-09-29T07:02:39+0000\n",
      "Posts scraped 127\n",
      "PostID: 85891659096_10152259246294097 from 2013-09-28T07:51:03+0000\n",
      "Posts scraped 128\n",
      "PostID: 85891659096_10152249948309097 from 2013-09-24T09:32:17+0000\n",
      "Posts scraped 129\n",
      "PostID: 85891659096_10152249941169097 from 2013-09-24T09:01:26+0000\n",
      "Posts scraped 130\n",
      "PostID: 85891659096_10152245995384097 from 2013-09-22T18:57:52+0000\n",
      "Posts scraped 131\n",
      "PostID: 85891659096_10152236027024097 from 2013-09-19T07:20:56+0000\n",
      "Posts scraped 132\n",
      "PostID: 85891659096_10152226945899097 from 2013-09-15T18:29:36+0000\n",
      "Posts scraped 133\n",
      "PostID: 85891659096_10152226916359097 from 2013-09-15T18:15:55+0000\n",
      "Posts scraped 134\n",
      "PostID: 85891659096_10152221547449097 from 2013-09-13T11:30:27+0000\n",
      "Posts scraped 135\n",
      "PostID: 85891659096_10152219381549097 from 2013-09-12T11:55:27+0000\n",
      "Posts scraped 136\n",
      "PostID: 85891659096_10152217455554097 from 2013-09-11T13:59:52+0000\n",
      "Posts scraped 137\n",
      "PostID: 85891659096_10152215111989097 from 2013-09-10T14:38:05+0000\n",
      "Posts scraped 138\n",
      "PostID: 85891659096_10152212000184097 from 2013-09-09T11:11:33+0000\n",
      "Posts scraped 139\n",
      "PostID: 85891659096_10152204104799097 from 2013-09-06T07:42:17+0000\n",
      "Posts scraped 140\n",
      "Fetching data\n",
      "PostID: 85891659096_10152204041189097 from 2013-09-06T06:34:28+0000\n",
      "Posts scraped 141\n",
      "PostID: 85891659096_10152201969829097 from 2013-09-05T08:51:50+0000\n",
      "Posts scraped 142\n",
      "PostID: 85891659096_10152198673739097 from 2013-09-03T21:59:37+0000\n",
      "Posts scraped 143\n",
      "PostID: 85891659096_10152194761809097 from 2013-09-02T15:00:19+0000\n",
      "Posts scraped 144\n",
      "PostID: 85891659096_10152185764309097 from 2013-08-29T19:41:20+0000\n",
      "Posts scraped 145\n",
      "PostID: 85891659096_10152180251504097 from 2013-08-27T11:47:53+0000\n",
      "Posts scraped 146\n",
      "PostID: 85891659096_10152178230819097 from 2013-08-26T16:06:38+0000\n",
      "Posts scraped 147\n",
      "PostID: 85891659096_10152175343414097 from 2013-08-25T15:27:19+0000\n",
      "Posts scraped 148\n",
      "PostID: 85891659096_10152170165609097 from 2013-08-23T15:41:23+0000\n",
      "Posts scraped 149\n",
      "PostID: 85891659096_10152169482574097 from 2013-08-23T06:40:17+0000\n",
      "Posts scraped 150\n",
      "PostID: 85891659096_10152167238944097 from 2013-08-22T08:35:30+0000\n",
      "Posts scraped 151\n",
      "PostID: 85891659096_10152161627419097 from 2013-08-19T23:16:39+0000\n",
      "Posts scraped 152\n",
      "PostID: 85891659096_10152151015744097 from 2013-08-15T21:10:07+0000\n",
      "Posts scraped 153\n",
      "PostID: 85891659096_10152149970859097 from 2013-08-15T12:55:12+0000\n",
      "Posts scraped 154\n",
      "PostID: 85891659096_10152149156239097 from 2013-08-15T04:53:53+0000\n",
      "Posts scraped 155\n",
      "PostID: 85891659096_10152148253624097 from 2013-08-14T21:51:16+0000\n",
      "Posts scraped 156\n",
      "PostID: 85891659096_10152127623804097 from 2013-08-06T11:28:29+0000\n",
      "Posts scraped 157\n",
      "PostID: 85891659096_10152121942104097 from 2013-08-04T08:17:05+0000\n",
      "Posts scraped 158\n",
      "PostID: 85891659096_10152116364134097 from 2013-08-01T22:25:52+0000\n",
      "Posts scraped 159\n",
      "PostID: 85891659096_10152112077624097 from 2013-07-31T08:13:16+0000\n",
      "Posts scraped 160\n",
      "Fetching data\n",
      "PostID: 85891659096_10152077991457995 from 2013-07-25T07:25:59+0000\n",
      "Posts scraped 161\n",
      "PostID: 85891659096_10152077991167995 from 2013-07-25T07:25:44+0000\n",
      "Posts scraped 162\n",
      "PostID: 85891659096_10152096797314097 from 2013-07-25T06:46:24+0000\n",
      "Posts scraped 163\n",
      "PostID: 85891659096_10152079972014097 from 2013-07-18T15:36:00+0000\n",
      "Posts scraped 164\n",
      "PostID: 85891659096_10152079969589097 from 2013-07-18T15:35:30+0000\n",
      "Posts scraped 165\n",
      "PostID: 85891659096_10151543041902831 from 2013-07-06T18:03:03+0000\n",
      "Posts scraped 166\n",
      "PostID: 85891659096_10152047335829097 from 2013-07-04T09:44:23+0000\n",
      "Posts scraped 167\n",
      "PostID: 85891659096_10152041805469097 from 2013-07-02T06:13:21+0000\n",
      "Posts scraped 168\n",
      "PostID: 85891659096_10152032599584097 from 2013-06-28T08:31:30+0000\n",
      "Posts scraped 169\n",
      "PostID: 85891659096_10152014817219097 from 2013-06-20T10:04:25+0000\n",
      "Posts scraped 170\n",
      "PostID: 85891659096_10151998953669097 from 2013-06-13T13:42:45+0000\n",
      "Posts scraped 171\n",
      "PostID: 85891659096_10151986656724097 from 2013-06-08T05:20:35+0000\n",
      "Posts scraped 172\n",
      "PostID: 85891659096_10151969836869097 from 2013-05-31T12:53:07+0000\n",
      "Posts scraped 173\n",
      "PostID: 85891659096_10151968520129097 from 2013-05-30T19:09:57+0000\n",
      "Posts scraped 174\n",
      "PostID: 85891659096_10151953033984097 from 2013-05-23T07:11:58+0000\n",
      "Posts scraped 175\n",
      "PostID: 85891659096_10151952441394097 from 2013-05-22T23:37:54+0000\n",
      "Posts scraped 176\n",
      "PostID: 85891659096_10151939229269097 from 2013-05-16T11:45:57+0000\n",
      "Posts scraped 177\n",
      "PostID: 85891659096_10151938224349097 from 2013-05-15T22:20:55+0000\n",
      "Posts scraped 178\n",
      "PostID: 85891659096_10151937124574097 from 2013-05-15T10:19:54+0000\n",
      "Posts scraped 179\n",
      "PostID: 85891659096_10151926397019097 from 2013-05-10T07:46:05+0000\n",
      "Posts scraped 180\n",
      "Fetching data\n",
      "PostID: 85891659096_10151925701764097 from 2013-05-09T21:26:22+0000\n",
      "Posts scraped 181\n",
      "PostID: 85891659096_10151923071154097 from 2013-05-08T12:08:49+0000\n",
      "Posts scraped 182\n",
      "PostID: 85891659096_10151922977299097 from 2013-05-08T10:29:14+0000\n",
      "Posts scraped 183\n",
      "PostID: 85891659096_10151913771939097 from 2013-05-03T17:35:32+0000\n",
      "Posts scraped 184\n",
      "PostID: 85891659096_10151912036194097 from 2013-05-02T19:17:31+0000\n",
      "Posts scraped 185\n",
      "PostID: 85891659096_652565831426972 from 2013-04-30T12:16:08+0000\n",
      "Posts scraped 186\n",
      "PostID: 85891659096_10151907207569097 from 2013-04-30T11:13:20+0000\n",
      "Posts scraped 187\n",
      "PostID: 85891659096_10151897557419097 from 2013-04-25T06:03:59+0000\n",
      "Posts scraped 188\n",
      "PostID: 85891659096_10151875961412995 from 2013-04-24T17:46:39+0000\n",
      "Posts scraped 189\n",
      "PostID: 85891659096_10151894922314097 from 2013-04-23T20:39:16+0000\n",
      "Posts scraped 190\n",
      "PostID: 85891659096_10151894140494097 from 2013-04-23T11:22:16+0000\n",
      "Posts scraped 191\n",
      "PostID: 85891659096_10151887116069097 from 2013-04-19T18:11:44+0000\n",
      "Posts scraped 192\n",
      "PostID: 85891659096_10151886452679097 from 2013-04-19T08:32:23+0000\n",
      "Posts scraped 193\n",
      "PostID: 85891659096_10151530766349420 from 2013-04-18T15:48:17+0000\n",
      "Posts scraped 194\n",
      "PostID: 85891659096_10151884739139097 from 2013-04-18T11:29:13+0000\n",
      "Posts scraped 195\n",
      "PostID: 85891659096_263809710423308 from 2013-04-11T14:58:38+0000\n",
      "Posts scraped 196\n",
      "PostID: 85891659096_10151866879304097 from 2013-04-08T19:03:03+0000\n",
      "Posts scraped 197\n",
      "PostID: 85891659096_554748971213911 from 2013-04-08T18:01:06+0000\n",
      "Posts scraped 198\n",
      "PostID: 85891659096_244044095739622 from 2013-04-05T13:56:40+0000\n",
      "Posts scraped 199\n",
      "PostID: 85891659096_268044366665373 from 2013-04-04T12:22:05+0000\n",
      "Posts scraped 200\n",
      "Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostID: 85891659096_10151857364739097 from 2013-04-03T11:44:15+0000\n",
      "Posts scraped 201\n",
      "PostID: 85891659096_154722114688734 from 2013-04-01T22:29:45+0000\n",
      "Posts scraped 202\n",
      "PostID: 85891659096_356264387817171 from 2013-03-28T09:21:37+0000\n",
      "Posts scraped 203\n",
      "PostID: 85891659096_361162360654408 from 2013-03-27T09:47:45+0000\n",
      "Posts scraped 204\n",
      "PostID: 85891659096_479812895407402 from 2013-03-25T11:10:46+0000\n",
      "Posts scraped 205\n",
      "PostID: 85891659096_547347235285423 from 2013-03-22T20:41:05+0000\n",
      "Posts scraped 206\n",
      "PostID: 85891659096_110219815815100 from 2013-03-21T18:48:08+0000\n",
      "Posts scraped 207\n",
      "PostID: 85891659096_398529736911692 from 2013-03-20T19:27:08+0000\n",
      "Posts scraped 208\n",
      "PostID: 85891659096_10151831489639097 from 2013-03-20T18:50:33+0000\n",
      "Posts scraped 209\n",
      "PostID: 85891659096_431656883585166 from 2013-03-14T20:49:26+0000\n",
      "Posts scraped 210\n",
      "PostID: 85891659096_342399785861734 from 2013-03-07T12:23:17+0000\n",
      "Posts scraped 211\n",
      "PostID: 85891659096_116756285177729 from 2013-03-06T21:32:54+0000\n",
      "Posts scraped 212\n",
      "PostID: 85891659096_486305094764594 from 2013-03-05T18:48:56+0000\n",
      "Posts scraped 213\n",
      "PostID: 85891659096_486133661447101 from 2013-03-04T14:15:08+0000\n",
      "Posts scraped 214\n",
      "PostID: 85891659096_10151791086549097 from 2013-03-04T12:14:08+0000\n",
      "Posts scraped 215\n",
      "PostID: 85891659096_416450145112102 from 2013-02-28T22:17:35+0000\n",
      "Posts scraped 216\n",
      "PostID: 85891659096_151233711702798 from 2013-02-28T15:06:57+0000\n",
      "Posts scraped 217\n",
      "PostID: 85891659096_262144850586923 from 2013-02-26T22:29:11+0000\n",
      "Posts scraped 218\n",
      "PostID: 85891659096_10151775877564097 from 2013-02-25T08:05:55+0000\n",
      "Posts scraped 219\n",
      "PostID: 85891659096_10151775817504097 from 2013-02-25T07:27:04+0000\n",
      "Posts scraped 220\n",
      "Fetching data\n",
      "PostID: 85891659096_144496309046078 from 2013-02-22T19:08:41+0000\n",
      "Posts scraped 221\n",
      "PostID: 85891659096_130846107092531 from 2013-02-21T23:08:06+0000\n",
      "Posts scraped 222\n",
      "PostID: 85891659096_338942689543280 from 2013-02-19T17:35:29+0000\n",
      "Posts scraped 223\n",
      "PostID: 85891659096_426902104059831 from 2013-02-18T21:43:37+0000\n",
      "Posts scraped 224\n",
      "PostID: 85891659096_10151761687129097 from 2013-02-18T13:40:16+0000\n",
      "Posts scraped 225\n",
      "PostID: 85891659096_470131529707958 from 2013-02-15T09:37:45+0000\n",
      "Posts scraped 226\n",
      "PostID: 85891659096_105888489594675 from 2013-02-12T21:58:26+0000\n",
      "Posts scraped 227\n",
      "PostID: 85891659096_158209684332794 from 2013-02-12T15:21:09+0000\n",
      "Posts scraped 228\n",
      "PostID: 85891659096_285604438234705 from 2013-02-12T12:12:56+0000\n",
      "Posts scraped 229\n",
      "PostID: 85891659096_218404358297731 from 2013-02-12T11:20:50+0000\n",
      "Posts scraped 230\n",
      "PostID: 85891659096_416977135043324 from 2013-02-11T18:45:41+0000\n",
      "Posts scraped 231\n",
      "PostID: 85891659096_519195751458652 from 2013-02-09T10:12:20+0000\n",
      "Posts scraped 232\n",
      "PostID: 85891659096_373045676126105 from 2013-02-08T17:52:07+0000\n",
      "Posts scraped 233\n",
      "PostID: 85891659096_288291267966469 from 2013-02-08T14:14:32+0000\n",
      "Posts scraped 234\n",
      "PostID: 85891659096_10151732729154097 from 2013-02-06T02:58:24+0000\n",
      "Posts scraped 235\n",
      "PostID: 85891659096_586164038079298 from 2013-02-05T18:41:13+0000\n",
      "Posts scraped 236\n",
      "PostID: 85891659096_10151730940894097 from 2013-02-05T08:38:26+0000\n",
      "Posts scraped 237\n",
      "PostID: 85891659096_517609688262388 from 2013-01-31T16:24:08+0000\n",
      "Posts scraped 238\n",
      "PostID: 85891659096_10151679490354097 from 2013-01-25T17:55:14+0000\n",
      "Posts scraped 239\n",
      "PostID: 85891659096_10151678097629097 from 2013-01-24T23:31:42+0000\n",
      "Posts scraped 240\n",
      "Fetching data\n",
      "PostID: 85891659096_10151677399444097 from 2013-01-24T15:05:50+0000\n",
      "Posts scraped 241\n",
      "PostID: 85891659096_519154698106634 from 2013-01-24T11:12:26+0000\n",
      "Posts scraped 242\n",
      "PostID: 85891659096_10151675547634097 from 2013-01-23T15:12:31+0000\n",
      "Posts scraped 243\n",
      "PostID: 85891659096_427566397314145 from 2013-01-23T13:35:40+0000\n",
      "Posts scraped 244\n",
      "PostID: 85891659096_246674025466491 from 2013-01-16T12:56:49+0000\n",
      "Posts scraped 245\n",
      "PostID: 85891659096_466776610046897 from 2013-01-15T16:20:49+0000\n",
      "Posts scraped 246\n",
      "PostID: 85891659096_133251873505726 from 2013-01-15T16:07:27+0000\n",
      "Posts scraped 247\n",
      "PostID: 85891659096_446260992095081 from 2013-01-04T18:20:08+0000\n",
      "Posts scraped 248\n",
      "PostID: 85891659096_10151634464249097 from 2013-01-03T07:51:36+0000\n",
      "Posts scraped 249\n",
      "PostID: 85891659096_103160819860068 from 2013-01-02T13:41:57+0000\n",
      "Posts scraped 250\n",
      "PostID: 85891659096_10151629304539097 from 2012-12-31T23:35:25+0000\n",
      "Posts scraped 251\n",
      "PostID: 85891659096_528545703832106 from 2012-12-28T13:42:38+0000\n",
      "Posts scraped 252\n",
      "PostID: 85891659096_10151615779199097 from 2012-12-25T12:57:09+0000\n",
      "Posts scraped 253\n",
      "PostID: 85891659096_10151615770319097 from 2012-12-25T12:41:25+0000\n",
      "Posts scraped 254\n",
      "PostID: 85891659096_352904604808718 from 2012-12-21T18:20:38+0000\n",
      "Posts scraped 255\n",
      "PostID: 85891659096_135922053231272 from 2012-12-20T13:27:02+0000\n",
      "Posts scraped 256\n",
      "PostID: 85891659096_3992517742552 from 2012-12-20T10:05:24+0000\n",
      "Posts scraped 257\n",
      "PostID: 85891659096_260376534089941 from 2012-12-19T14:48:07+0000\n",
      "Posts scraped 258\n",
      "PostID: 85891659096_555537317808872 from 2012-12-18T17:49:44+0000\n",
      "Posts scraped 259\n",
      "PostID: 85891659096_501546139890192 from 2012-12-18T17:14:12+0000\n",
      "Posts scraped 260\n",
      "Fetching data\n",
      "PostID: 85891659096_451944468195277 from 2012-12-14T14:06:23+0000\n",
      "Posts scraped 261\n",
      "PostID: 85891659096_10151593419564097 from 2012-12-13T19:13:26+0000\n",
      "Posts scraped 262\n",
      "PostID: 85891659096_572543036106087 from 2012-12-13T19:05:49+0000\n",
      "Posts scraped 263\n",
      "PostID: 85891659096_335484969892731 from 2012-12-11T18:07:28+0000\n",
      "Posts scraped 264\n",
      "PostID: 85891659096_10151587650284097 from 2012-12-10T12:33:15+0000\n",
      "Posts scraped 265\n",
      "PostID: 85891659096_133698480119897 from 2012-12-09T09:40:51+0000\n",
      "Posts scraped 266\n",
      "PostID: 85891659096_10151581954444097 from 2012-12-07T08:06:11+0000\n",
      "Posts scraped 267\n",
      "PostID: 85891659096_10151580918014097 from 2012-12-06T17:56:39+0000\n",
      "Posts scraped 268\n",
      "PostID: 85891659096_444943168902769 from 2012-12-05T08:19:49+0000\n",
      "Posts scraped 269\n",
      "PostID: 85891659096_447006062023266 from 2012-12-04T18:00:03+0000\n",
      "Posts scraped 270\n",
      "PostID: 85891659096_475090002542647 from 2012-12-04T09:31:47+0000\n",
      "Posts scraped 271\n",
      "PostID: 85891659096_529078613778696 from 2012-12-03T09:44:23+0000\n",
      "Posts scraped 272\n",
      "PostID: 85891659096_448451141879480 from 2012-11-30T12:28:32+0000\n",
      "Posts scraped 273\n",
      "PostID: 85891659096_301592599957528 from 2012-11-30T12:18:52+0000\n",
      "Posts scraped 274\n",
      "PostID: 85891659096_10151567858464097 from 2012-11-29T06:49:49+0000\n",
      "Posts scraped 275\n",
      "PostID: 85891659096_498535416844004 from 2012-11-28T12:42:09+0000\n",
      "Posts scraped 276\n",
      "PostID: 85891659096_554972214516369 from 2012-11-27T09:14:13+0000\n",
      "Posts scraped 277\n",
      "PostID: 85891659096_4093598220787 from 2012-11-26T09:33:03+0000\n",
      "Posts scraped 278\n",
      "PostID: 85891659096_501064463259643 from 2012-11-23T22:02:13+0000\n",
      "Posts scraped 279\n",
      "PostID: 85891659096_494704627216391 from 2012-11-22T14:35:04+0000\n",
      "Posts scraped 280\n",
      "Fetching data\n",
      "PostID: 85891659096_10151553208924097 from 2012-11-20T15:46:28+0000\n",
      "Posts scraped 281\n",
      "PostID: 85891659096_431990093515806 from 2012-11-20T10:01:48+0000\n",
      "Posts scraped 282\n",
      "PostID: 85891659096_372927722792511 from 2012-11-17T13:31:52+0000\n",
      "Posts scraped 283\n",
      "PostID: 85891659096_10151547742484097 from 2012-11-17T11:54:59+0000\n",
      "Posts scraped 284\n",
      "PostID: 85891659096_525684067442801 from 2012-11-15T12:49:37+0000\n",
      "Posts scraped 285\n",
      "PostID: 85891659096_10151532746654097 from 2012-11-08T10:05:06+0000\n",
      "Posts scraped 286\n",
      "PostID: 85891659096_10151532686049097 from 2012-11-08T09:01:06+0000\n",
      "Posts scraped 287\n",
      "PostID: 85891659096_351102801652926 from 2012-11-08T08:06:53+0000\n",
      "Posts scraped 288\n",
      "PostID: 85891659096_371016702985548 from 2012-11-06T11:56:26+0000\n",
      "Posts scraped 289\n",
      "PostID: 85891659096_277157609071349 from 2012-11-05T16:10:18+0000\n",
      "Posts scraped 290\n",
      "PostID: 85891659096_10151525912104097 from 2012-11-04T16:27:28+0000\n",
      "Posts scraped 291\n",
      "PostID: 85891659096_549521235075114 from 2012-11-01T18:08:10+0000\n",
      "Posts scraped 292\n",
      "PostID: 85891659096_10151518560369097 from 2012-10-31T12:12:28+0000\n",
      "Posts scraped 293\n",
      "PostID: 85891659096_300808963362379 from 2012-10-31T12:02:29+0000\n",
      "Posts scraped 294\n",
      "PostID: 85891659096_10151510501269097 from 2012-10-27T09:44:54+0000\n",
      "Posts scraped 295\n",
      "PostID: 85891659096_161661887309195 from 2012-10-24T13:00:58+0000\n",
      "Posts scraped 296\n",
      "PostID: 85891659096_296628777108242 from 2012-10-23T14:15:53+0000\n",
      "Posts scraped 297\n",
      "PostID: 85891659096_111291325696490 from 2012-10-22T09:37:49+0000\n",
      "Posts scraped 298\n",
      "PostID: 85891659096_530403993640611 from 2012-10-16T06:29:02+0000\n",
      "Posts scraped 299\n",
      "PostID: 85891659096_293668817401990 from 2012-10-16T06:13:28+0000\n",
      "Posts scraped 300\n",
      "Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostID: 85891659096_195854930549666 from 2012-10-15T09:05:33+0000\n",
      "Posts scraped 301\n",
      "PostID: 85891659096_289625067819247 from 2012-10-11T18:10:30+0000\n",
      "Posts scraped 302\n",
      "PostID: 85891659096_10151475548124097 from 2012-10-07T15:54:39+0000\n",
      "Posts scraped 303\n",
      "PostID: 85891659096_289704234469712 from 2012-10-06T08:42:15+0000\n",
      "Posts scraped 304\n",
      "PostID: 85891659096_267062893396454 from 2012-10-02T18:44:53+0000\n",
      "Posts scraped 305\n",
      "PostID: 85891659096_449342261771186 from 2012-09-27T19:13:21+0000\n",
      "Posts scraped 306\n",
      "PostID: 85891659096_10151459052234097 from 2012-09-27T16:05:11+0000\n",
      "Posts scraped 307\n",
      "PostID: 85891659096_10151457856974097 from 2012-09-26T20:14:01+0000\n",
      "Posts scraped 308\n",
      "PostID: 85891659096_420676231322282 from 2012-09-26T12:36:46+0000\n",
      "Posts scraped 309\n",
      "PostID: 85891659096_10151455229359097 from 2012-09-25T05:36:31+0000\n",
      "Posts scraped 310\n",
      "PostID: 85891659096_10151452826829097 from 2012-09-23T20:44:37+0000\n",
      "Posts scraped 311\n",
      "PostID: 85891659096_421452137912224 from 2012-09-23T08:13:40+0000\n",
      "Posts scraped 312\n",
      "PostID: 85891659096_440893195963225 from 2012-09-23T08:09:35+0000\n",
      "Posts scraped 313\n",
      "PostID: 85891659096_340759769347354 from 2012-09-17T11:29:41+0000\n",
      "Posts scraped 314\n",
      "PostID: 85891659096_10151415850567995 from 2012-09-15T19:39:51+0000\n",
      "Posts scraped 315\n",
      "PostID: 85891659096_10151440100474097 from 2012-09-15T19:27:24+0000\n",
      "Posts scraped 316\n",
      "PostID: 85891659096_413907225325196 from 2012-09-14T08:47:53+0000\n",
      "Posts scraped 317\n",
      "PostID: 85891659096_354810777933720 from 2012-09-13T15:08:15+0000\n",
      "Posts scraped 318\n",
      "PostID: 85891659096_415316721861074 from 2012-09-10T19:04:52+0000\n",
      "Posts scraped 319\n",
      "PostID: 85891659096_454345721277432 from 2012-09-05T11:19:18+0000\n",
      "Posts scraped 320\n",
      "Fetching data\n",
      "PostID: 85891659096_399144433473041 from 2012-08-31T12:25:05+0000\n",
      "Posts scraped 321\n",
      "PostID: 85891659096_10151370674239097 from 2012-08-14T09:02:16+0000\n",
      "Posts scraped 322\n",
      "PostID: 85891659096_147035698768059 from 2012-08-13T00:11:09+0000\n",
      "Posts scraped 323\n",
      "PostID: 85891659096_10151365827279097 from 2012-08-12T14:54:42+0000\n",
      "Posts scraped 324\n",
      "PostID: 85891659096_10151360079394097 from 2012-08-10T08:24:14+0000\n",
      "Posts scraped 325\n",
      "PostID: 85891659096_10151357806614097 from 2012-08-09T11:18:08+0000\n",
      "Posts scraped 326\n",
      "PostID: 85891659096_213760325416606 from 2012-08-02T13:06:29+0000\n",
      "Posts scraped 327\n",
      "PostID: 85891659096_10151236962049097 from 2012-06-22T11:30:15+0000\n",
      "Posts scraped 328\n",
      "PostID: 85891659096_209984862456703 from 2012-06-21T12:08:27+0000\n",
      "Posts scraped 329\n",
      "PostID: 85891659096_448340968511947 from 2012-06-01T13:16:29+0000\n",
      "Posts scraped 330\n",
      "PostID: 85891659096_408930662484533 from 2012-05-29T07:19:45+0000\n",
      "Posts scraped 331\n",
      "PostID: 85891659096_10151165097434097 from 2012-05-25T12:02:17+0000\n",
      "Posts scraped 332\n",
      "PostID: 85891659096_10151162183859097 from 2012-05-24T09:22:19+0000\n",
      "Posts scraped 333\n",
      "PostID: 85891659096_100214740117455 from 2012-05-11T13:27:09+0000\n",
      "Posts scraped 334\n",
      "PostID: 85891659096_372462292788862 from 2012-04-27T10:56:09+0000\n",
      "Posts scraped 335\n",
      "PostID: 85891659096_10151005331964097 from 2012-04-26T17:09:25+0000\n",
      "Posts scraped 336\n",
      "PostID: 85891659096_10151004806264097 from 2012-04-26T13:16:51+0000\n",
      "Posts scraped 337\n",
      "PostID: 85891659096_10151004801994097 from 2012-04-26T13:14:37+0000\n",
      "Posts scraped 338\n",
      "PostID: 85891659096_293587007387324 from 2012-04-19T11:17:19+0000\n",
      "Posts scraped 339\n",
      "PostID: 85891659096_10150959018999097 from 2012-04-13T06:36:42+0000\n",
      "Posts scraped 340\n",
      "Fetching data\n",
      "PostID: 85891659096_142919769156592 from 2012-04-12T06:48:59+0000\n",
      "Posts scraped 341\n",
      "PostID: 85891659096_358976847479156 from 2012-03-30T11:18:57+0000\n",
      "Posts scraped 342\n",
      "PostID: 85891659096_10150921725419097 from 2012-03-29T19:39:47+0000\n",
      "Posts scraped 343\n",
      "PostID: 85891659096_254489484643164 from 2012-03-21T11:13:37+0000\n",
      "Posts scraped 344\n",
      "PostID: 85891659096_322215864504615 from 2012-03-20T13:17:32+0000\n",
      "Posts scraped 345\n",
      "PostID: 85891659096_287136378023991 from 2012-03-12T12:17:03+0000\n",
      "Posts scraped 346\n",
      "PostID: 85891659096_120995158022826 from 2012-02-03T12:08:37+0000\n",
      "Posts scraped 347\n",
      "PostID: 85891659096_10150719867859097 from 2012-01-16T12:29:55+0000\n",
      "Posts scraped 348\n",
      "85891659096 with 348 posts scraped in: 22.76 seconds. Time is: 19:47\n",
      "Scraping 1394162844242435 page 2 of 3\n",
      "Fetching data\n",
      "Feed data empty. Skipping ID\n",
      "Scraping 1300642593330497 page 3 of 3\n",
      "Fetching data\n",
      "Problem with GRAPH GET. Lowering limit\n",
      "Limit was: 20\n",
      "Limit is now: 10\n",
      "Something went wrong, skipping 1300642593330497. ID was saved in missing_test_01-01-2012_31-12-2013.json\n",
      "Scraping of test.json ended at 19:47 after 0.0 minutes\n",
      "error sending mail\n",
      "error sending mail\n",
      "Scraping done\n"
     ]
    }
   ],
   "source": [
    "#Changes: \n",
    "    #- The script automatically generates a folder for harvest jsons. \n",
    "    #- The script generates a json called missing where all the skipped IDs are entered no matter where in the script things went wrong.\n",
    "    \n",
    "#Remember to:  \n",
    "    #- Change the name of the seedlist. Dont call it .json. It will be appended. The seedlist must be in the rootfolder\n",
    "    #- Adjust the start and endtime, and limits\n",
    "    #- Insert email address for notifications (remember to look in the spamfolder)\n",
    "\n",
    "#prerequisites:\n",
    "    #- Seedlist must be in a flat .json structure as a list [\"FB_ID_1\",\" FB_ID_2\", etc]\n",
    "\n",
    "import sys\n",
    "import smtplib\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import facebook\n",
    "except:\n",
    "    print('Facebook-sdk not installed. Installing...')\n",
    "    install_facebook()\n",
    "\n",
    "import facebook\n",
    "\n",
    "############Uncomment or create a new seedlist_name without .json############\n",
    "\n",
    "#seedlist_name='missing_missing_DK-FB-Page-Race-PoliticiansSnowball_01-01-2012_31-12-2013_01-01-2012_31-12-2013'\n",
    "seedlist_name='test'\n",
    "\n",
    "############Insert appID and secret############\n",
    "\n",
    "appID= '1056141877822551' #Backscatter original app\n",
    "appsecret='ea9c62c22fed73bbf5aedba6b2b7843c'##Backscatter original app\n",
    "\n",
    "\n",
    "############Adjust limits############\n",
    "\n",
    "postlimit_start=20 #set to 80\n",
    "reactionlimit='600' #max 1000\n",
    "commentlimit= '40' #max 100\n",
    "\n",
    "############Adjust start and end dates###########\n",
    "startdate='01-01-2012'\n",
    "enddate='31-12-2013'\n",
    "\n",
    "############Mail setup###########\n",
    "mail_address_1=\"\"\n",
    "mail_address_2=\"\"\n",
    "mails = [mail_address_1, mail_address_2]#'recipient@mailservice.com'\n",
    "gmail_sender = 'motherscraper@gmail.com'\n",
    "gmail_passwd = 'scrapeyourmother'\n",
    "email_notifications=1 #1 for notifications and 0 for none\n",
    "\n",
    "\n",
    "\n",
    "############Other initial definitions##########\n",
    "seedlist=seedlist_name+'.json'\n",
    "#seedlist=seedlist_name+'.json'\n",
    "sleep_time=0.1\n",
    "graph = facebook.GraphAPI(access_token=appID+'|'+appsecret, version='2.7')\n",
    "start_time_total=time.time()\n",
    "\n",
    "\n",
    "##########Create harvest folder##########\n",
    "if not os.path.exists(seedlist_name):\n",
    "    os.makedirs(seedlist_name)\n",
    "    \n",
    "##########Create json for skipped IDs#############    \n",
    "\n",
    "missing='missing_'+seedlist_name+'_'+startdate+'_'+enddate+'.json'\n",
    "\n",
    "if not os.path.exists(missing):\n",
    "    with open(missing, 'w') as outfile:\n",
    "        json.dump([], outfile, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "def send_mail(TO, SUBJECT, TEXT):\n",
    "\n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.login(gmail_sender, gmail_passwd)\n",
    "\n",
    "    BODY = '\\r\\n'.join(['To: %s' % TO,\n",
    "                        'From: %s' % gmail_sender,\n",
    "                        'Subject: %s' % SUBJECT,\n",
    "                        '', TEXT])\n",
    "\n",
    "    try:\n",
    "        server.sendmail(gmail_sender, [TO], BODY)\n",
    "        print('email sent to '+TO)\n",
    "    except:\n",
    "        print ('error sending mail')\n",
    "\n",
    "    server.quit()        \n",
    "\n",
    "def install_facebook():\n",
    "    !{sys.executable} -m pip install facebook-sdk\n",
    "    print('Facebook-sdk installed. Importing...')\n",
    "    import facebook\n",
    "    print('Facebook imported')\n",
    "        \n",
    "def skip_id(missing_data):\n",
    "    with open(missing) as f:\n",
    "        data=json.load(f)\n",
    "    data.append(missing_data)\n",
    "    with open(missing, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def paging_postreactions(): #comment_reactions or post_reactions or commentcomment_reactions\n",
    "    connection_r=1\n",
    "    reactionpaging_token=reactions['paging']\n",
    "    while 'next' in reactionpaging_token and connection_r:\n",
    "        reactionpaging_token=reactionpaging_token['next']\n",
    "        reactionpaging_token = reactionpaging_token.split('/')\n",
    "        reactionpaging_token = '/' + reactionpaging_token[4] + '/' + reactionpaging_token[5]\n",
    "        reactionpaging_data=graph.get_object(id=reactionpaging_token)\n",
    "        try:\n",
    "            reactionpaging_data=graph.get_object(id=reactionpaging_token)\n",
    "            for reaction in reactionpaging_data['data']:\n",
    "                comment_reactions.append(reaction)\n",
    "            reactionpaging_token=reactionpaging_data['paging']\n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            print('Problems with comments comments reactions.. Skipping ID')\n",
    "            connection_r=0\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "def paging_commentreactions(): #comment_reactions or post_reactions or commentcomment_reactions\n",
    "    connection_cr=1\n",
    "    reactionpaging_token=reactions['paging']\n",
    "    while 'next' in reactionpaging_token and connection_cr:\n",
    "        reactionpaging_token=reactionpaging_token['next']\n",
    "        reactionpaging_token = reactionpaging_token.split('/')\n",
    "        reactionpaging_token = '/' + reactionpaging_token[4] + '/' + reactionpaging_token[5]\n",
    "        try:\n",
    "            reactionpaging_data=graph.get_object(id=reactionpaging_token)\n",
    "            for reaction in reactionpaging_data['data']:\n",
    "                comment_reactions.append(reaction)\n",
    "            reactionpaging_token=reactionpaging_data['paging']\n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            print('Problems with comments reactions.. Skipping ID')\n",
    "            connection_cr=0\n",
    "            continue\n",
    "\n",
    "def paging_commentcommentreactions(): #comment_reactions or post_reactions or commentcomment_reactions\n",
    "    connection_ccr=1\n",
    "    reactionpaging_token=reactions['paging']\n",
    "    print(\"paging comment comment reaction\")\n",
    "    while 'next' in reactionpaging_token and connection_ccr:\n",
    "        reactionpaging_token=reactionpaging_token['next']\n",
    "        reactionpaging_token = reactionpaging_token.split('/')\n",
    "        reactionpaging_token = '/' + reactionpaging_token[4] + '/' + reactionpaging_token[5]\n",
    "        try:\n",
    "            reactionpaging_data=graph.get_object(id=reactionpaging_token)\n",
    "            for reaction in reactionpaging_data['data']:\n",
    "                commentcomment_reactions.append(reaction)\n",
    "            reactionpaging_token=reactionpaging_data['paging']\n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            print('Problems with comments comments reactions.. Skipping ID')\n",
    "            connection_ccr=0\n",
    "            continue\n",
    "\n",
    "\n",
    "def paging_commentcomment(): \n",
    "    connection_commentcomment=1\n",
    "    commentcommentpaging_token=commentcomments['paging']\n",
    "    while 'next' in commentcommentpaging_token and connection_commentcomment:\n",
    "        commentcommentpaging_token=commentcommentpaging_token['next']\n",
    "        commentcommentpaging_token = commentcommentpaging_token.split('/')\n",
    "        commentcommentpaging_token = '/' + commentcommentpaging_token[4] + '/' + commentcommentpaging_token[5]\n",
    "        try:\n",
    "            \n",
    "            commentcommentpaging_data=graph.get_object(id=commentcommentpaging_token)\n",
    "            for commentcomment in commentcommentpaging_data['data']:\n",
    "                commentcomment_time = commentcomment['created_time']\n",
    "                commentcomment_id = commentcomment['id']\n",
    "                commentcomment_author = commentcomment['from']\n",
    "                if 'message' in commentcomment:\n",
    "                    commentcomment_message = commentcomment['message']\n",
    "                else:\n",
    "                    commentcomment_message = ''\n",
    "\n",
    "                #iteratae over commentcomment reactions\n",
    "                commentcomment_reactions=[]\n",
    "                if 'reactions' in commentcomment:\n",
    "                    reactions = commentcomment['reactions']\n",
    "                    for reaction in reactions['data']:\n",
    "                        commentcomment_reactions.append(reaction)\n",
    "                    paging_commentcommentreactions()\n",
    "\n",
    "\n",
    "                COMMENTCOMMENTDIC = {'commentcomment_time':commentcomment_time,'commentcomment_id':commentcomment_id,'commentcomment_message':commentcomment_message,'commentcomment_author':commentcomment_author,'commentcomment_reactions':commentcomment_reactions}\n",
    "                commentcomments_data.append(COMMENTCOMMENTDIC)\n",
    "            time.sleep(sleep_time)\n",
    "            commentcommentpaging_token=commentcommentpaging_data['paging']\n",
    "        except:\n",
    "            print('Problems with comment paging.. Skipping ID')\n",
    "            connection_commentcomment=0\n",
    "            continue\n",
    "       \n",
    "\n",
    "            \n",
    "def paging_comment(): #commentcomment or comment\n",
    "    connection_comment=1\n",
    "    y=1\n",
    "    commentpaging_token=comments['paging']\n",
    "    while 'next' in commentpaging_token and y<11 and connection_comment:\n",
    "        commentpaging_token=commentpaging_token['next']\n",
    "        commentpaging_token = commentpaging_token.split('/')\n",
    "        commentpaging_token = '/' + commentpaging_token[4] + '/' + commentpaging_token[5]\n",
    "        try:\n",
    "            \n",
    "            commentpaging_data=graph.get_object(id=commentpaging_token)\n",
    "            for comment in commentpaging_data['data']:\n",
    "\n",
    "                comment_time = comment['created_time']\n",
    "                comment_id = comment['id']\n",
    "                comment_author = comment['from']\n",
    "                if 'message' in comment:\n",
    "                    comment_message = comment['message']\n",
    "                else:\n",
    "                    comment_message = ''\n",
    "                comment_reactions=[]\n",
    "                if 'reactions' in comment:\n",
    "                        reactions = comment['reactions'] \n",
    "                        for reaction in reactions['data']:\n",
    "                            comment_reactions.append(reaction)\n",
    "                        paging_commentreactions()\n",
    "                commentcomments_data=[]\n",
    "                if 'comment' in comment: \n",
    "                    commentcomment=comment['comments']\n",
    "                    print(commentcomment)\n",
    "\n",
    "                    paging_commentcomment()\n",
    "                COMMENTDIC = {'comment_time':comment_time,'comment_id':comment_id,'comment_message':comment_message,'comment_author':comment_author,'comment_reactions':comment_reactions,'commentcomments':commentcomments_data}\n",
    "                comments_data.append(COMMENTDIC)\n",
    "            time.sleep(sleep_time)\n",
    "            commentpaging_token=commentpaging_data['paging']\n",
    "            y=y+1\n",
    "        except:\n",
    "            print('Problems with comment paging.. Skipping ID')\n",
    "            connection_comment=0\n",
    "            continue\n",
    "\n",
    "\n",
    "x=0\n",
    "data = json.load(open(seedlist))\n",
    "data=set(data)\n",
    "data=list(data)\n",
    "all_FB_ID=len(data)\n",
    "now = time.strftime(\"%H:%M\", time.localtime(time.time()))\n",
    "if all_FB_ID>60:\n",
    "    estimated_duration=str(round((all_FB_ID/60)/3,0))+' hours' \n",
    "else:\n",
    "    estimated_duration=str(round((all_FB_ID/3),0))+' minutes'\n",
    "    \n",
    "if email_notifications:\n",
    "    for mail in mails:\n",
    "        send_mail(mail, \"Motherscraper started \"+str(now), \"Motherscraper started scraping from: \"+seedlist+\". Estimated duration: \"+estimated_duration)\n",
    "else:\n",
    "    print('Email not send')\n",
    "try:\n",
    "    while x<all_FB_ID:\n",
    "       # data = json.load(open(seedlist))\n",
    "        start_time=time.time()\n",
    "        FB_ID=data[x]\n",
    "        print('Scraping '+FB_ID+' page '+str(x+1)+' of '+ str(all_FB_ID))\n",
    "        postlimit=postlimit_start\n",
    "\n",
    "        filepath=seedlist_name+'/'+FB_ID+'_'+startdate+'_to_'+enddate+'.json'\n",
    "        with open(filepath, 'w') as outfile:\n",
    "            json.dump([], outfile, ensure_ascii=False)\n",
    "\n",
    "        post_count=0\n",
    "        try: \n",
    "            print('Fetching data')\n",
    "            feed_data = graph.get_object(FB_ID+'/feed?fields=from,created_time,link,picture,message,story,scheduled_publish_time,reactions.limit('+reactionlimit+'){id,type},comments.limit('+commentlimit+'){comments.limit('+commentlimit+'){from,created_time,message,id,reactions.limit('+reactionlimit+')},from,created_time,message,id,reactions.limit('+reactionlimit+')}&limit='+str(postlimit)+'&since='+startdate+'&until='+enddate)\n",
    "\n",
    "\n",
    "        except:\n",
    "            print('Problem with GRAPH GET. Lowering limit')\n",
    "            print('Limit was: '+str(postlimit))\n",
    "            postlimit=postlimit-10\n",
    "            print('Limit is now: '+str(postlimit))\n",
    "            try:\n",
    "                feed_data = graph.get_object(FB_ID+'/feed?fields=from,created_time,link,picture,message,story,scheduled_publish_time,reactions.limit('+reactionlimit+'){id,type},comments.limit('+commentlimit+'){comments.limit('+commentlimit+'){from,created_time,message,id,reactions.limit('+reactionlimit+')},from,created_time,message,id,reactions.limit('+reactionlimit+')}&limit='+str(postlimit)+'&since='+startdate+'&until='+enddate)\n",
    "            except:\n",
    "                print('Something went wrong, skipping '+FB_ID+'. ID was saved in '+missing)\n",
    "                x=x+1\n",
    "                skip_id(FB_ID)\n",
    "                continue\n",
    "        if not feed_data['data']:\n",
    "            print('Feed data empty. Skipping ID')\n",
    "            x=x+1\n",
    "            continue\n",
    "        else:\n",
    "            x=x+1\n",
    "        DATA = []\n",
    "\n",
    "        for post in feed_data['data']:\n",
    "            post_time = post['created_time']\n",
    "            post_id = post['id']\n",
    "            post_author = post['from']\n",
    "            print('PostID: '+ str(post_id)+' from '+post_time)\n",
    "            if 'message' in post:\n",
    "                post_message = post['message']\n",
    "            else:\n",
    "                post_message = ''\n",
    "            if 'picture' in post:\n",
    "                post_picture = post['picture']\n",
    "            else:\n",
    "                post_picture = ''\n",
    "            if 'link' in post:\n",
    "                post_link = post['link']\n",
    "            else:\n",
    "                post_link = ''\n",
    "\n",
    "            #iterate over post reactions\n",
    "            post_reactions = []\n",
    "            if 'reactions' in post:\n",
    "                reactions = post['reactions']\n",
    "                for reaction in reactions['data']:\n",
    "                    post_reactions.append(reaction)\n",
    "                paging_postreactions()\n",
    "\n",
    "            #iterate over comments\n",
    "            comments_data = []\n",
    "            if 'comments' in post:\n",
    "                comments = post['comments']\n",
    "                for comment in comments['data']: \n",
    "                    comment_time = comment['created_time']\n",
    "                    comment_id = comment['id']\n",
    "                    comment_author = comment['from']\n",
    "                    if 'message' in comment:\n",
    "                        comment_message = comment['message']\n",
    "                    else:\n",
    "                        comment_message = ''\n",
    "\n",
    "                    #iterate over comment reactions\n",
    "                    comment_reactions = []\n",
    "                    if 'reactions' in comment:\n",
    "                        reactions = comment['reactions']\n",
    "                        for reaction in reactions['data']:\n",
    "                            comment_reactions.append(reaction)\n",
    "                        paging_commentreactions()\n",
    "\n",
    "                    #iterate over comments on comments\n",
    "                    commentcomments_data = []\n",
    "                    if 'comments' in comment:\n",
    "                        commentcomments = comment['comments']\n",
    "                        for commentcomment in commentcomments['data']:\n",
    "                            commentcomment_time = commentcomment['created_time']\n",
    "                            commentcomment_id = commentcomment['id']\n",
    "                            commentcomment_author = commentcomment['from']\n",
    "                            if 'message' in commentcomment:\n",
    "                                commentcomment_message = commentcomment['message']\n",
    "                            else:\n",
    "                                commentcomment_message = ''\n",
    "\n",
    "                            #iteratae over commentcomment reactions\n",
    "                            commentcomment_reactions = []\n",
    "                            if 'reactions' in commentcomments:\n",
    "                                reactions = commentcomment['reactions']\n",
    "                                for reaction in reactions['data']:\n",
    "                                    commentcomment_reactions.append(reaction)\n",
    "                                paging_commentcommentreactions()\n",
    "\n",
    "                            COMMENTCOMMENTDIC = {'commentcomment_time':commentcomment_time,'commentcomment_id':commentcomment_id,'commentcomment_message':commentcomment_message,'commentcomment_author':commentcomment_author,'commentcomment_reactions':commentcomment_reactions}\n",
    "                            commentcomments_data.append(COMMENTCOMMENTDIC)\n",
    "                        paging_commentcomment()\n",
    "\n",
    "\n",
    "\n",
    "                    COMMENTDIC = {'comment_time':comment_time,'comment_id':comment_id,'comment_message':comment_message,'comment_author':comment_author,'comment_reactions':comment_reactions,'commentcomments':commentcomments_data}\n",
    "                    comments_data.append(COMMENTDIC)\n",
    "                paging_comment()\n",
    "            POSTDATA = {'post_id':post_id,'post_time':post_time,'post_message':post_message,'post_author':post_author,'post_picture':post_picture,'post_link':post_link,'post_reactions':post_reactions,'comments':comments_data}\n",
    "\n",
    "            DATA.append(POSTDATA)\n",
    "\n",
    "\n",
    "            post_count=post_count+1\n",
    "            print('Posts scraped '+str(post_count))\n",
    "\n",
    "        print(\"Finished first post page.\")\n",
    "        if 'next' in feed_data['paging']:\n",
    "            print('Paging posts for '+FB_ID)\n",
    "        connection_page=1\n",
    "\n",
    "\n",
    "        #########################POST PAGING############################\n",
    "        try:\n",
    "            while 'next' in feed_data['paging'] and connection_page:\n",
    "                post_paging_token = feed_data['paging']['next']\n",
    "                post_paging_token = post_paging_token.split('/')\n",
    "                post_paging_token = '/' + post_paging_token[4] + '/' + post_paging_token[5]\n",
    "                try: \n",
    "                    print('Fetching data')\n",
    "                    feed_data = graph.get_object(id=post_paging_token)\n",
    "\n",
    "                except:\n",
    "                    print('Problem with GRAPH GET. Lowering limit')\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        time.sleep(1)\n",
    "                        limit=postlimit-10\n",
    "                        print('Postlimit was: '+str(postlimit)+' is now: '+str(limit))\n",
    "                        post_paging_token = post_paging_token.split('&')\n",
    "                        new_post_paging_token=post_paging_token[0] + '&' + post_paging_token[1] + '&' + post_paging_token[2]+ '&' + post_paging_token[3]+ '&limit='+str(limit)+'&' + post_paging_token[5]            \n",
    "                        feed_data = graph.get_object(id=new_post_paging_token)\n",
    "\n",
    "                    except:    \n",
    "                        print('Still problems with GRAPH GET. Lowering limit again') \n",
    "                        try:\n",
    "                            time.sleep(1)\n",
    "                            limit_2=10\n",
    "                            print('Postlimit was: '+str(limit)+' is now: '+str(limit_2))\n",
    "                            post_paging_token = post_paging_token.split('&')\n",
    "                            new_post_paging_token=post_paging_token[0] + '&' + post_paging_token[1] + '&' + post_paging_token[2]+ '&' + post_paging_token[3]+ '&limit='+str(limit_2)+'&' + post_paging_token[5]\n",
    "                            feed_data = graph.get_object(id=new_post_paging_token)\n",
    "                        except:\n",
    "                            print('Could not get paging data. Stopping paging for '+FB_ID+'. ID was saved in missing.json.')\n",
    "                            skip_id(FB_ID)\n",
    "                            connection_page=0\n",
    "                            continue\n",
    "\n",
    "                for post in feed_data['data']:\n",
    "                    post_time = post['created_time']\n",
    "                    post_id = post['id']\n",
    "                    post_author = post['from']\n",
    "                    print('PostID: '+ str(post_id)+' from '+post_time)\n",
    "\n",
    "                    if 'message' in post:\n",
    "                        post_message = post['message']\n",
    "                    else:\n",
    "                        post_message = ''\n",
    "                    if 'picture' in post:\n",
    "                        post_picture = post['picture']\n",
    "                    else:\n",
    "                        post_picture = ''\n",
    "                    if 'link' in post:\n",
    "                        post_link = post['link']\n",
    "                    else:\n",
    "                        post_link = ''\n",
    "\n",
    "                    #iterate over post reactions\n",
    "                    post_reactions = []\n",
    "                    if 'reactions' in post:\n",
    "                        reactions = post['reactions']\n",
    "                        for reaction in reactions['data']:\n",
    "                            post_reactions.append(reaction)\n",
    "                        paging_postreactions()\n",
    "\n",
    "                    #iterate over comments\n",
    "                    comments_data = []\n",
    "                    if 'comments' in post:\n",
    "                        comments = post['comments']\n",
    "                        for comment in comments['data']: \n",
    "                            comment_time = comment['created_time']\n",
    "                            comment_id = comment['id']\n",
    "                            comment_author = comment['from']\n",
    "                            if 'message' in comment:\n",
    "                                comment_message = comment['message']\n",
    "                            else:\n",
    "                                comment_message = ''\n",
    "\n",
    "                            #iterate over comment reactions\n",
    "                            comment_reactions = []\n",
    "                            if 'reactions' in comment:\n",
    "                                reactions = comment['reactions']\n",
    "                                for reaction in reactions['data']:\n",
    "                                    comment_reactions.append(reaction)\n",
    "                                paging_commentreactions()\n",
    "\n",
    "                            #iterate over comments on comments\n",
    "                            commentcomments_data = []\n",
    "                            if 'comments' in comment:\n",
    "\n",
    "                                commentcomments = comment['comments']\n",
    "                                for commentcomment in commentcomments['data']:\n",
    "                                    commentcomment_time = commentcomment['created_time']\n",
    "                                    commentcomment_id = commentcomment['id']\n",
    "                                    commentcomment_author = commentcomment['from']\n",
    "                                    if 'message' in commentcomment:\n",
    "                                        commentcomment_message = commentcomment['message']\n",
    "                                    else:\n",
    "                                        commentcomment_message = ''\n",
    "\n",
    "                                    #iteratae over commentcomment reactions\n",
    "                                    commentcomment_reactions = []\n",
    "                                    if 'reactions' in commentcomments:\n",
    "                                        reactions = commentcomment['reactions']\n",
    "                                        for reaction in reactions['data']:\n",
    "                                            commentcomment_reactions.append(reaction)\n",
    "                                        paging_commentcommentreactions()\n",
    "\n",
    "                                    COMMENTCOMMENTDIC = {'commentcomment_time':commentcomment_time,'commentcomment_id':commentcomment_id,'commentcomment_message':commentcomment_message,'commentcomment_author':commentcomment_author,'commentcomment_reactions':commentcomment_reactions}\n",
    "                                    commentcomments_data.append(COMMENTCOMMENTDIC)\n",
    "                                paging_commentcomment()\n",
    "\n",
    "\n",
    "\n",
    "                            COMMENTDIC = {'comment_time':comment_time,'comment_id':comment_id,'comment_message':comment_message,'comment_author':comment_author,'comment_reactions':comment_reactions,'commentcomments':commentcomments_data}\n",
    "                            comments_data.append(COMMENTDIC)\n",
    "                        paging_comment()\n",
    "                    POSTDATA = {'post_id':post_id,'post_time':post_time,'post_message':post_message,'post_author':post_author,'post_picture':post_picture,'post_link':post_link,'post_reactions':post_reactions,'comments':comments_data}\n",
    "\n",
    "                    DATA.append(POSTDATA)    \n",
    "\n",
    "                    post_count=post_count+1\n",
    "                    print('Posts scraped '+str(post_count)) \n",
    "        except:\n",
    "            skip_id(FB_ID)\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(DATA, f)\n",
    "            continue\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(DATA, f)\n",
    "\n",
    "        end_time=time.time()\n",
    "        duration=end_time-start_time \n",
    "        now = time.strftime(\"%H:%M\", time.localtime(time.time()))\n",
    "        print(str(FB_ID)+' with '+str(post_count)+' posts scraped in: '+str(round(duration,2))+' seconds. Time is: '+str(now))\n",
    "\n",
    "    end_time_total=time.time()\n",
    "    duration_total=round(((end_time_total-start_time_total)/60),0)\n",
    "    now = time.strftime(\"%H:%M\", time.localtime(time.time()))\n",
    "\n",
    "    print('Scraping of '+seedlist+ ' ended at '+str(now)+' after '+str(duration_total)+' minutes')\n",
    "    if email_notifications:\n",
    "        for mail in mails:\n",
    "            send_mail(mail,\"Motherscraper finished \"+str(now), \"Motherscraper successfully finished scraping \"+seedlist+\" in \"+ str(duration_total)+\" minutes\")\n",
    "except Exception as e: \n",
    "    print('Problem in the script: '+str(e)+' \\nPlease edit the seedlist to start at '+str(FB_ID)+' and restart the script')\n",
    "    if email_notifications:\n",
    "        for mail in mails:\n",
    "            send_mail(mail,\"Motherscraper has unexpectedly stopped \"+str(now), \"Motherscraper scraping from \"+seedlist+\" has unexspectedly stopped at \"+str(FB_ID))\n",
    "\n",
    "print('Scraping done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
